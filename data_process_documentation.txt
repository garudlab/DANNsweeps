# check num of individuals 
plink --bfile v40.3.BA --freq --out tmp/freq_BA
grep "people" freq_BA.log # should give 177

#check average missing data per site
plink --bfile v51.0.H --missing --out tmp/missing_data_H
awk 'NR > 1 { total += $5; count++ } END { print total/count }' tmp/missing_data_H.lmiss

#convert to vcf
plink --bfile v51.0.H --recode vcf --out tmp/data_H.vcf

#extract chrom 2 from epoch H 

plink --bfile v51.0.H --chr 2 --make-bed --out ../Chr2_H/v51.0.H.chr2

#convert to vcf in /u/project/ngarud/Garud_lab/aDNA/Chr2_H
plink --bfile v51.0.H.chr2 --recode vcf --out VCF.v51.0.H.chr2


## do this for all chromsomes

for chr in {1..22}; do
    #extract chrom
    plink --bfile ../DANN_DATA/v40.3.M --chr $chr --make-bed --out v40.3.M.chr$chr

    #convert to vcf
    plink --bfile v40.3.M.chr$chr --recode vcf --out VCF.v40.3.M.chr$chr

    rm v40.3.M.chr$chr*
    rm *log

done

for chr in {1..22}; do
    #extract chrom
    plink --bfile  DANN_DATA/YRI/v40.3.YRI --chr $chr --make-bed --out v40.3.YRI.chr$chr

    #convert to vcf
    plink --bfile v40.3.YRI.chr$chr --recode vcf --out YRI/VCF.v40.3.YRI.chr$chr

    rm -f v40.3.YRI.chr$chr*
    rm -f *log

done




#extract region that includes LCT
plink --bfile v51.0.H --chr 2 --from-bp 135408034 --to-bp 136308034 --make-bed --out ../Chr2_H/v51.0.H.chr2.LCT
#convert to vcf in /u/project/ngarud/Garud_lab/aDNA/Chr2_H
plink --bfile v51.0.H.chr2.LCT --recode vcf --out VCF.v51.0.H.chr2.LCT

# HLA region IA
plink --bfile v51.0.IA --chr 6 --from-bp 28138809 --to-bp 28910235 --make-bed --out ../Chr6_IA/v51.0.IA.chr6.HLA
#convert to vcf in /u/project/ngarud/Garud_lab/aDNA/Chr2_H
plink --bfile v51.0.IA.chr6.HLA --recode vcf --out VCF.v51.0.IA.chr6.HLA

#extract random region chrom2
plink --bfile v51.0.H --chr 2 --from-bp 203169 --to-bp 1103169 --make-bed --out ../Chr2_H/v51.0.H.chr2.region
#convert to vcf in /u/project/ngarud/Garud_lab/aDNA/Chr2_H
plink --bfile v51.0.H.chr2.region --recode vcf --out VCF.v51.0.H.chr2.region



#Simulations add segsites:201 
sed -i 's/segsites: [0-9]\+/segsites: 201/g' MDSSbottleneck50_1e4_sb0.1_H0.5_THETA5_PF0.75_target1_Q50_A.txt


for (( i=8; i<=22; i++ )); do
    echo $i
    # Add your commands here that use $i as needed
    plink --bfile v51.0.H.chr${i} --recode vcf --out VCF.v51.0.H.chr${i}
done



# to extract everything in a file up until a line that starts with // appears n times

awk '
  BEGIN { count = 0 }
  /^\/\// { count++ }
  count < 100001 { print }
  count == 100001 { exit }
' MDSScteNe_1e4_sb0.1_H0.5_THETA5.0_PF0.95_target0_Q50_A.txt > MDSScteNe_1e4_sb0.1_H0.5_THETA5.0_PF0.95_target0_Q50_1.txt


# last n //  lines

awk '
  BEGIN { count = 0; print_flag = 0 }
  /^\/\// { count++ }
  count >= 100001 { print_flag = 1 }
  print_flag { print }
' MDSScteNe_1e4_sb0.1_H0.5_THETA5.0_PF0.95_target0_Q50_A.txt > MDSScteNe_1e4_sb0.1_H0.5_THETA5.0_PF0.95_target0_Q50_2.txt



#merge SGV data
counter=1
# Concaten
for i in $(seq 1 2 50); do
    echo $counter
    cat MD_SGVSweep_1e6_H0.5_Q50_A_$i.txt MD_SGVSweep_1e6_H0.5_Q50_A_$((i+1)).txt  > MD_SGVSweep_1e6_H0.5_THETAvary_Q50_merged_$counter.txt
    # Increment the counter
    ((counter++))
done


#merge SGV and soft sweeps
for i in {1..20}; do
cat MD_SoftSweep_1e6_H0.5_THETAvary_Q50_merged_${i}.txt MD_SGVSweep_1e6_H0.5_THETAvary_Q50_merged_${i}.txt > MD_SoftandSGVSweep_1e6_H0.5_THETAvary_Q50_merged_${i}.txt
done

#remove first 400 samples so that I have total of 10K samples per file
for i in {1..20}; do
awk '
  BEGIN { count = 0; print_flag = 0 }
  /^\/\// { count++ }
  count >= 401 { print_flag = 1 }
  print_flag { print }
' MD_SoftandSGVSweep_1e6_H0.5_THETAvary_Q50_merged_${i}.txt > MD_SoftandSGVSweep_1e6_H0.5_THETAvary_Q50_merged_${i}.txt_tmp 
done

for i in {1..20}; do
mv MD_SoftandSGVSweep_1e6_H0.5_THETAvary_Q50_merged_${i}.txt_tmp MD_SoftandSGVSweep_1e6_H0.5_THETAvary_Q50_merged_${i}.txt
done

################ simulation processing 

1) Make sure you have correct number of segsites:

for file in *; do
    sed -i 's/segsites: [0-9]\+/segsites: 201/g' "$file"
done


2) Rename each file in format Model_THETA_target.txt


for i in {1..40}; do
    mv MD_Neutral_1e6_sb0_Q50_A_${i}.txt NeutralMD_THETA0.0_target0_${i}.txt
    #mv MD_SoftSweep_1e4_H0.5_THETAvary_Q1_450Kb_${i}.txt  SoftSweepMD_THETA5.0_target0_${i}.txt
    #mv MD_HardSweep_1e4_H0.5_THETA0.01_Q1_450Kb_${i}.txt  HardSweepMD_THETA0.01_target0_${i}.txt
done



for file in *; do
    grep "segsites" "$file" | wc -l
done


#merge soft sweep data
counter=1
# Loop through files in groups of 5
for i in $(seq 1 5 100); do
    # Concatenate 5 files at a time
    echo $counter
    cat SoftSweep_1e6_H0.5_THETAvary_Q50_$i.txt SoftSweep_1e6_H0.5_THETAvary_Q50_$((i+1)).txt SoftSweep_1e6_H0.5_THETAvary_Q50_$((i+2)).txt SoftSweep_1e6_H0.5_THETAvary_Q50_$((i+3)).txt SoftSweep_1e6_H0.5_THETAvary_Q50_$((i+4)).txt > SoftSweep_1e6_H0.5_THETAvary_Q50_merged_$counter.txt
    # Increment the counter
    ((counter++))
done

################ scan processing 
for chr in {1..22};do
    python ../addRecomb.py -i chr${chr}_H_predCNN_j10_05.txt -o chr${chr}_H_predCNN_j10_05_rec.txt -g "/u/project/ngarud/Garud_lab/aDNA/RecombMaps/DeCodeSexAveraged_GRCh37/genetic_map_decode_sex-averaged_chr${chr}.txt"

    #rm chr${chr}_H_pred_j10_08.txt
done


#add constant chrom number to file

for chr in {1..22};do
    awk -v num="$chr" '{print $0 "\t" num}' chr${chr}_H_predCNN_j10_05_rec.txt > chr${chr}_H_predCNN_j10_05_rec_tmp.txt
    mv chr${chr}_H_predCNN_j10_05_rec_tmp.txt chr${chr}_H_predCNN_j10_05_rec.txt
done

cat chr*H*_rec.txt > scanCNN_RowDist_H_j10_05_rec.txt




##### GET TOP PEAKS VCF AND ANNOTION

# get full vcf
plink --bfile v51.0.H --recode vcf --out VCFfiles/v51.0.H

#sort 
bcftools query -f '%CHROM\t%POS\t%POS\t%ID\n' v51.0.H.vcf >  v51.0.H.bed
mv peaks_H_RowDist.txt peaks_H_RowDist.bed

# filter common chromosomes
awk '{print $1}' peaks_H_RowDist.bed | sort -u > query_chroms.txt
awk '{print $1}' v51.0.H.bed | sort -u > db_chroms.txt
comm -12 query_chroms.txt db_chroms.txt > common_chroms.txt

#  filter BED files:
grep -wFf common_chroms.txt peaks_H_RowDist.bed > filtered_query.bed
grep -wFf common_chroms.txt v51.0.H.bed > filtered_db.bed

mv filtered_query.bed peaks_H_RowDist.bed
mv filtered_db.bed v51.0.H.bed

bedtools closest -a peaks_H_RowDist.bed -b v51.0.H.bed -d > topPeaks_v51.0.H.bed

#get positons to filter VCF
cut -f4,5 topPeaks_v51.0.H.bed > filter_positions.txt

#get subset VCF
bgzip v51.0.H.vcf
bcftools index v51.0.H.vcf.gz

bcftools view -R filter_positions.txt v51.0.H.vcf.gz -Oz -o topPeaks_v51.0.H.vcf.gz



#### Find closest genes
#I used VEP to get genses in 256000 windows upstream and downstream of variant

grep "protein_coding" VEP_Hpeaks.txt > VEP_Hpeaks_proteinCoding.txt



##using bedtools only
wget "https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_42/GRCh37_mapping/gencode.v42lift37.annotation.gtf.gz"
zgrep 'gene_type "protein_coding"' gencode.v42lift37.annotation.gtf > protein_coding.gtf

module load bedops

awk '{ if ($0 ~ "transcript_id") print $0; else print $0" transcript_id \"\";"; }' protein_coding.gtf | gtf2bed - > protein_coding.bed

#sort files
#bedtools sort -i peaks_H_RowDist.bed > sorted_peaks_H_RowDist.bed
bedtools sort -i protein_coding.bed > genes_sorted.bed

sed -i 's/chr//g' genes_sorted.bed 

grep "protein_coding" genes_sorted.bed > genes_sorted_proteinCoding.bed

bedtools closest -a peaks_H_RowDist.bed -b genes_sorted_proteinCoding.bed -D a -t first > closest_genes.tsv


bedtools closest -a peaks_H_RowDist.bed -b genes_sorted_proteinCoding.bed -D a -t first \
  | awk 'BEGIN {OFS="\t"} {print $1,$2,$3,$4,$18,$NF}' > closest_genes.tsv


###### Visualize peaks

module load htslib
module load bcftools


#21592609-22105845
#72584990-73282330
#109284096-109624296
#135756777-136631071 #LCT
#152521376-152940018
#167860886-168320237
#178235617-178600535
#197233200-197723715

#Neutral
#29970016-30630912

bgzip -c VCF.v51.0.H.chr2.vcf > VCF.v51.0.H.chr2.vcf.gz
tabix -p vcf VCF.v51.0.H.chr2.vcf.gz
bcftools view -r 2:29970016-30630912 VCF.v51.0.H.chr2.vcf.gz -o chr2.peakH12.vcf

qsub qsub_DataForViz #edit file path accordingly
